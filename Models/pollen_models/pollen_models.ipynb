{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T04:06:07.592592Z",
     "iopub.status.busy": "2025-12-10T04:06:07.592218Z",
     "iopub.status.idle": "2025-12-10T04:06:08.838647Z",
     "shell.execute_reply": "2025-12-10T04:06:08.838243Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# load datasets\n",
    "weather = pd.read_csv(\"Data/boston-weather-data(open_meteo).csv\", skiprows=3)\n",
    "print(weather.columns.tolist())\n",
    "pollution = pd.read_csv(\"Data/boston_pollutants_with_aqi_include_2024.csv\")\n",
    "pollen = pd.read_csv(\"Data/EPHT_Pollen_Data.csv\")\n",
    "\n",
    "# parse date columns\n",
    "weather['time'] = pd.to_datetime(weather['time'])\n",
    "pollution['date'] = pd.to_datetime(pollution['date'])\n",
    "pollen['Date'] = pd.to_datetime(pollen['Date'])\n",
    "\n",
    "# rename and merge\n",
    "weather.rename(columns={'time': 'Date'}, inplace=True)\n",
    "pollution.rename(columns={'date': 'Date'}, inplace=True)\n",
    "\n",
    "merged = (\n",
    "    weather\n",
    "    .merge(pollution, on='Date', how='inner')\n",
    "    .merge(pollen, on='Date', how='inner')\n",
    ")\n",
    "\n",
    "merged.info()\n",
    "\n",
    "# filter for pollen season (March through October)\n",
    "merged = merged[merged['Date'].dt.month.isin(range(3, 11))]\n",
    "\n",
    "\n",
    "# select relevant numeric columns\n",
    "cols = [\n",
    "    # Weather (all numeric)\n",
    "    'weather_code (wmo code)',\n",
    "    'temperature_2m_mean (Â°C)',\n",
    "    'apparent_temperature_mean (Â°C)',\n",
    "    'precipitation_sum (mm)',\n",
    "    'wind_gusts_10m_max (km/h)',\n",
    "    'wind_speed_10m_max (km/h)',\n",
    "    'wind_direction_10m_dominant (Â°)',\n",
    "\n",
    "    # Pollution\n",
    "    'PM2.5', 'O3', 'CO', 'NO2', 'SO2',\n",
    "    'AQI_PM2.5', 'AQI_O3', 'AQI_CO', 'AQI_NO2', 'AQI_SO2',\n",
    "    'AQI', 'num_pollutants_available',\n",
    "\n",
    "    # Pollen\n",
    "    'Tree', 'Grass', 'Weed', 'Ragweed', 'Total_Pollen'\n",
    "]\n",
    "\n",
    "numeric = merged[[c for c in cols if c in merged.columns]]\n",
    "corr = numeric.corr()\n",
    "\n",
    "\n",
    "# ------------------------------\n",
    "# Pollen statistics\n",
    "# ------------------------------\n",
    "pollen_cols = ['Tree', 'Grass', 'Weed', 'Ragweed', 'Total_Pollen']\n",
    "\n",
    "for col in pollen_cols:\n",
    "    if col in merged.columns:\n",
    "        print(f\"{col} range: {merged[col].min()} - {merged[col].max()} | mean: {merged[col].mean():.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Linear Regression Model - Not Very Good\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T04:06:08.859213Z",
     "iopub.status.busy": "2025-12-10T04:06:08.859000Z",
     "iopub.status.idle": "2025-12-10T04:06:09.393339Z",
     "shell.execute_reply": "2025-12-10T04:06:09.392939Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# use the merged dataframe\n",
    "df = merged.copy()\n",
    "\n",
    "# drop rows with missing values in predictors or target\n",
    "df = df.dropna(subset=numeric.columns)\n",
    "\n",
    "# ------------------------------\n",
    "# 1. Select target + features\n",
    "# ------------------------------\n",
    "target = \"Total_Pollen\"   \n",
    "remove_cols = [\"Tree\", \"Grass\", \"Weed\", \"Ragweed\", \"Total_Pollen\"] # remove pollen related variables \n",
    "features = [c for c in numeric.columns if c not in remove_cols]\n",
    "\n",
    "# ------------------------------\n",
    "# 2. Split data by date (all 2024 as test)\n",
    "# ------------------------------\n",
    "df = df.sort_values(\"Date\")  # make sure dates are in order\n",
    "\n",
    "train_df = df[df['Year'] < 2023]   # all years before 2024 for training\n",
    "test_df = df[df['Year'] >= 2023]   # all of 2024 for testing\n",
    "\n",
    "print(df['Date'].min(), df['Date'].max())       # check the full date range\n",
    "print(df['Date'].dt.year.unique())             # see which years exist\n",
    "print(df[df['Date'].dt.year == 2024].shape)    # see how many rows for 2024\n",
    "\n",
    "X_train = train_df[features]\n",
    "y_train = train_df[target]\n",
    "\n",
    "X_test = test_df[features]\n",
    "y_test = test_df[target]\n",
    "\n",
    "# ------------------------------\n",
    "# 3. Scale features\n",
    "# ------------------------------\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# ------------------------------\n",
    "# 4. Train linear regression\n",
    "# ------------------------------\n",
    "model = LinearRegression()\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# ------------------------------\n",
    "# 5. Model evaluation\n",
    "# ------------------------------\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "\n",
    "print(\"MAE:\", mean_absolute_error(y_test, y_pred))\n",
    "print(\"RMSE:\", np.sqrt(mean_squared_error(y_test, y_pred)))\n",
    "print(\"RÂ²:\", r2_score(y_test, y_pred))\n",
    "\n",
    "# ------------------------------\n",
    "# 6. Coefficients (feature importance for linear regression)\n",
    "# ------------------------------\n",
    "coef_df = pd.DataFrame({\n",
    "    \"feature\": features,\n",
    "    \"coefficient\": model.coef_\n",
    "}).sort_values(by=\"coefficient\", key=abs, ascending=False)\n",
    "\n",
    "print(coef_df)\n",
    "\n",
    "# ------------------------------\n",
    "# 7. Visualize results\n",
    "# ------------------------------\n",
    "\n",
    "# Actual vs Predicted\n",
    "plt.figure(figsize=(6,6))\n",
    "plt.scatter(y_test, y_pred, alpha=0.6)\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')\n",
    "plt.xlabel(\"Actual Total Pollen\")\n",
    "plt.ylabel(\"Predicted Total Pollen\")\n",
    "plt.title(\"Actual vs Predicted Pollen (Linear Regression)\")\n",
    "plt.show()\n",
    "\n",
    "# Residuals plot\n",
    "residuals = y_test - y_pred\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.scatter(y_pred, residuals, alpha=0.6)\n",
    "plt.axhline(0, color='red', linestyle='--')\n",
    "plt.xlabel(\"Predicted Pollen\")\n",
    "plt.ylabel(\"Residuals\")\n",
    "plt.title(\"Residual Plot\")\n",
    "plt.show()\n",
    "\n",
    "# Feature importance (coefficients)\n",
    "plt.figure(figsize=(8,6))\n",
    "coef_df_sorted = coef_df.sort_values(\"coefficient\", key=abs)\n",
    "plt.barh(coef_df_sorted[\"feature\"], coef_df_sorted[\"coefficient\"])\n",
    "plt.xlabel(\"Coefficient Value\")\n",
    "plt.title(\"Linear Regression Feature Importance\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T04:06:09.395711Z",
     "iopub.status.busy": "2025-12-10T04:06:09.395474Z",
     "iopub.status.idle": "2025-12-10T04:06:10.867195Z",
     "shell.execute_reply": "2025-12-10T04:06:10.866745Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# -------------------------\n",
    "# CONFIG\n",
    "# -------------------------\n",
    "YEAR_SPLIT = 2023\n",
    "\n",
    "# -------------------------\n",
    "# 0. Basic checks and setup\n",
    "# -------------------------\n",
    "try:\n",
    "    df = merged.copy()\n",
    "    \n",
    "except NameError as e:\n",
    "    raise RuntimeError(\"DataFrame `merged` not found in environment. Please load it and try again.\") from e\n",
    "\n",
    "# Ensure Date column exists and is datetime\n",
    "if \"Date\" not in df.columns:\n",
    "    raise RuntimeError(\"`merged` must contain a 'Date' column.\")\n",
    "df = df.sort_values(\"Date\").reset_index(drop=True)\n",
    "df[\"Date\"] = pd.to_datetime(df[\"Date\"], errors=\"coerce\")\n",
    "\n",
    "if df[\"Date\"].isna().any():\n",
    "    raise RuntimeError(\"Some 'Date' values could not be parsed as datetime. Fix or drop them before running.\")\n",
    "\n",
    "# Basic target check\n",
    "if \"Total_Pollen\" not in df.columns:\n",
    "    raise RuntimeError(\"`merged` must contain 'Total_Pollen' as the target column.\")\n",
    "\n",
    "# Create Year for splitting\n",
    "df[\"Year\"] = df[\"Date\"].dt.year\n",
    "\n",
    "# -------------------------\n",
    "# 1. Numeric columns detection (robust)\n",
    "# -------------------------\n",
    "# Prefer user's `numeric` if present and appears valid\n",
    "numeric_cols = []\n",
    "try:\n",
    "    # numeric could be a DataFrame or Index-like\n",
    "    if 'numeric' in globals() or 'numeric' in locals():\n",
    "        # try to coerce to list of names\n",
    "        try:\n",
    "            numeric_cols = list(numeric.columns)\n",
    "        except Exception:\n",
    "            numeric_cols = list(numeric)\n",
    "except Exception:\n",
    "    numeric_cols = []\n",
    "\n",
    "# Fallback: infer numeric-like columns from df (exclude target/date/year and obviously non-feature names)\n",
    "if not numeric_cols:\n",
    "    numeric_cols = df.select_dtypes(include=[\"number\"]).columns.tolist()\n",
    "\n",
    "# Remove excluded technical columns if present\n",
    "exclude_if_present = {\"Total_Pollen\", \"Tree\", \"Grass\", \"Weed\", \"Ragweed\", \"Date\", \"Year\"}\n",
    "numeric_cols = [c for c in numeric_cols if c in df.columns and c not in exclude_if_present]\n",
    "\n",
    "print(f\"Using numeric cols (detected): {numeric_cols[:20]}{'...' if len(numeric_cols)>20 else ''}\")\n",
    "\n",
    "# Coerce detected numeric cols to numeric safely\n",
    "for col in numeric_cols:\n",
    "    df[col] = pd.to_numeric(df[col], errors=\"coerce\")\n",
    "\n",
    "# -------------------------\n",
    "# 2. Safe basic engineered features (only past info)\n",
    "# -------------------------\n",
    "# Target lags and rolling on shifted series (shift first, then rolling)\n",
    "df[\"lag1\"] = df[\"Total_Pollen\"].shift(1)\n",
    "df[\"lag2\"] = df[\"Total_Pollen\"].shift(2)\n",
    "df[\"lag3\"] = df[\"Total_Pollen\"].shift(3)\n",
    "\n",
    "# Past-only rolling means (shift before rolling)\n",
    "df[\"pollen_3day\"] = df[\"Total_Pollen\"].shift(1).rolling(window=3, min_periods=1).mean()\n",
    "df[\"pollen_7day\"] = df[\"Total_Pollen\"].shift(1).rolling(window=7, min_periods=1).mean()\n",
    "\n",
    "# Day-of-year & sin/cos\n",
    "df[\"day_of_year\"] = df[\"Date\"].dt.dayofyear\n",
    "df[\"sin_day\"] = np.sin(2 * np.pi * df[\"day_of_year\"] / 365)\n",
    "df[\"cos_day\"] = np.cos(2 * np.pi * df[\"day_of_year\"] / 365)\n",
    "\n",
    "# -------------------------\n",
    "# 3. Rolling & lag features for available environmental columns (no bfill)\n",
    "# -------------------------\n",
    "# Candidate names you might expect; we'll only keep the ones that exist in df.\n",
    "candidate_rolling_cols = [\n",
    "    \"temperature_2m_mean (Â°C)\", \"apparent_temperature_mean (Â°C)\",\n",
    "    \"PM2.5\", \"O3\", \"CO\", \"NO2\", \"SO2\", \"AQI\",\n",
    "    \"AQI_PM2.5\", \"AQI_O3\", \"AQI_CO\", \"AQI_NO2\", \"AQI_SO2\",\n",
    "    \"wind_speed_10m_max (km/h)\"\n",
    "]\n",
    "\n",
    "rolling_cols = [c for c in candidate_rolling_cols if c in df.columns]\n",
    "print(\"Found rolling columns:\", rolling_cols)\n",
    "\n",
    "weights = np.array([0.1, 0.3, 0.6])\n",
    "\n",
    "for col in rolling_cols:\n",
    "    s0 = df[col].shift(2)  # t-2\n",
    "    s1 = df[col].shift(1)  # t-1\n",
    "    s2 = df[col]           # t\n",
    "    df[f\"{col}_weighted3\"] = (weights[0] * s0) + (weights[1] * s1) + (weights[2] * s2)\n",
    "    # simple past lags\n",
    "    for lag in range(1, 4):\n",
    "        df[f\"{col}_lag{lag}\"] = df[col].shift(lag)\n",
    "\n",
    "# Interaction pairs â€” only create when both members exist\n",
    "candidate_interactions = [\n",
    "    (\"temperature_2m_mean (Â°C)\", \"AQI\"),\n",
    "    (\"apparent_temperature_mean (Â°C)\", \"PM2.5\"),\n",
    "    (\"wind_speed_10m_max (km/h)\", \"O3\"),\n",
    "]\n",
    "for c1, c2 in candidate_interactions:\n",
    "    if c1 in df.columns and c2 in df.columns:\n",
    "        df[f\"{c1}_x_{c2}\"] = df[c1] * df[c2]\n",
    "\n",
    "# -------------------------\n",
    "# 4. Safe spike indicator computed using TRAIN statistics only\n",
    "# -------------------------\n",
    "train_mask_for_threshold = df[\"Year\"] < YEAR_SPLIT\n",
    "# The threshold calculation itself is safe, as it uses only historical (train) data.\n",
    "train_pollen_series = df.loc[train_mask_for_threshold, \"Total_Pollen\"].dropna()\n",
    "\n",
    "if len(train_pollen_series) >= 5:\n",
    "    threshold = train_pollen_series.mean() + train_pollen_series.std()\n",
    "else:\n",
    "    # fallback to robust train median-based threshold\n",
    "    threshold = train_pollen_series.median() + 1.0 * train_pollen_series.std() if len(train_pollen_series) > 0 else df[\"Total_Pollen\"].median()\n",
    "print(\"Spike threshold (computed on training period):\", threshold)\n",
    "\n",
    "df[\"is_spike\"] = (df[\"Total_Pollen\"].shift(1) > threshold).astype(int)\n",
    "\n",
    "# ------------------------\n",
    "# 5. Build feature list (only features that exist)\n",
    "# -------------------------\n",
    "target = \"Total_Pollen\"\n",
    "remove_cols = {\"Tree\", \"Grass\", \"Weed\", \"Ragweed\", target, \"Date\", \"Year\"}\n",
    "\n",
    "base_features = [c for c in numeric_cols if c not in remove_cols]\n",
    "extra_features = [\"lag1\", \"lag2\", \"lag3\", \"pollen_3day\", \"pollen_7day\", \"sin_day\", \"cos_day\", \"is_spike\"]\n",
    "generated = [c for c in df.columns if (\"_weighted3\" in c) or (c.endswith(\"_lag1\") or c.endswith(\"_lag2\") or c.endswith(\"_lag3\")) or (\"_x_\" in c)]\n",
    "generated = [c for c in generated if c not in {\"lag1\", \"lag2\", \"lag3\"}]\n",
    "\n",
    "# Combine preserving order and existence\n",
    "features = []\n",
    "for c in base_features + extra_features + generated:\n",
    "    if c in df.columns and c not in features:\n",
    "        features.append(c)\n",
    "\n",
    "print(\"Final feature count:\", len(features))\n",
    "print(\"Some features:\", features[:40])\n",
    "\n",
    "# -------------------------\n",
    "# 6. Create X_all, y_all and drop rows without past info\n",
    "# -------------------------\n",
    "X_all = df[features].copy()\n",
    "y_all = df[target].copy()\n",
    "\n",
    "# Rows lacking core lag/rolling info must be removed (these are rows near the start)\n",
    "required_cols = [\"lag1\", \"lag2\", \"lag3\", \"pollen_3day\", \"pollen_7day\"]\n",
    "required_cols = [c for c in required_cols if c in X_all.columns]\n",
    "\n",
    "if not required_cols:\n",
    "    raise RuntimeError(\"No required lag features are present â€” check the dataset or feature construction.\")\n",
    "\n",
    "mask_valid = X_all[required_cols].notna().all(axis=1)\n",
    "n_dropped = (~mask_valid).sum()\n",
    "print(f\"Dropping {n_dropped} rows without required lag/rolling info (start of series).\")\n",
    "X_all = X_all.loc[mask_valid].reset_index(drop=True)\n",
    "y_all = y_all.loc[mask_valid].reset_index(drop=True)\n",
    "df = df.loc[mask_valid].reset_index(drop=True)  # keep aligned df for Year splitting\n",
    "\n",
    "# -------------------------\n",
    "# 7. Time-based train/test split (no leakage)\n",
    "# -------------------------\n",
    "train_idx = df[\"Year\"] < YEAR_SPLIT\n",
    "test_idx = df[\"Year\"] >= YEAR_SPLIT\n",
    "\n",
    "X_train = X_all.loc[train_idx].reset_index(drop=True)\n",
    "X_test  = X_all.loc[test_idx].reset_index(drop=True)\n",
    "y_train = y_all.loc[train_idx].reset_index(drop=True)\n",
    "y_test  = y_all.loc[test_idx].reset_index(drop=True)\n",
    "\n",
    "print(\"Shapes after split - X_train:\", X_train.shape, \"X_test:\", X_test.shape)\n",
    "\n",
    "# -------------------------\n",
    "# 8. Missing value imputation using TRAIN medians\n",
    "# -------------------------\n",
    "train_medians = X_train.median()\n",
    "X_train = X_train.fillna(train_medians)\n",
    "X_test  = X_test.fillna(train_medians)\n",
    "\n",
    "# If target has NaNs in either set remove those rows\n",
    "train_notnull = y_train.notna()\n",
    "test_notnull  = y_test.notna()\n",
    "\n",
    "X_train = X_train.loc[train_notnull].reset_index(drop=True)\n",
    "y_train = y_train.loc[train_notnull].reset_index(drop=True)\n",
    "\n",
    "X_test = X_test.loc[test_notnull].reset_index(drop=True)\n",
    "y_test = y_test.loc[test_notnull].reset_index(drop=True)\n",
    "\n",
    "# -------------------------\n",
    "# 9. Log transform target\n",
    "# -------------------------\n",
    "y_train_log = np.log1p(y_train)\n",
    "y_test_log  = np.log1p(y_test)\n",
    "\n",
    "# -------------------------\n",
    "# 10. Fit RandomForest (leakage-safe)\n",
    "# -------------------------\n",
    "rf = RandomForestRegressor(\n",
    "    n_estimators=200,\n",
    "    max_depth=None,\n",
    "    min_samples_split=2,\n",
    "    min_samples_leaf=4,\n",
    "    max_features=None,\n",
    "    bootstrap=True,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "rf.fit(X_train, y_train_log)\n",
    "\n",
    "# -------------------------\n",
    "# 11. Predict & invert log\n",
    "# -------------------------\n",
    "y_train_pred_log = rf.predict(X_train)\n",
    "y_test_pred_log  = rf.predict(X_test)\n",
    "\n",
    "y_train_pred = np.expm1(y_train_pred_log)\n",
    "y_test_pred  = np.expm1(y_test_pred_log)\n",
    "\n",
    "y_train_real = np.expm1(y_train_log)\n",
    "y_test_real  = np.expm1(y_test_log)\n",
    "\n",
    "# -------------------------\n",
    "# 12. Metrics\n",
    "# -------------------------\n",
    "print(\"\\nTraining Metrics:\")\n",
    "print(\"MAE:\", mean_absolute_error(y_train_real, y_train_pred))\n",
    "print(\"RMSE:\", np.sqrt(mean_squared_error(y_train_real, y_train_pred)))\n",
    "print(\"RÂ²:\", r2_score(y_train_real, y_train_pred))\n",
    "\n",
    "print(\"\\nTest Metrics:\")\n",
    "print(\"MAE:\", mean_absolute_error(y_test_real, y_test_pred))\n",
    "print(\"RMSE:\", np.sqrt(mean_squared_error(y_test_real, y_test_pred)))\n",
    "print(\"RÂ²:\", r2_score(y_test_real, y_test_pred))\n",
    "\n",
    "# -------------------------\n",
    "# 13. Feature importances\n",
    "# -------------------------\n",
    "importances = pd.DataFrame({\n",
    "    \"feature\": X_train.columns,\n",
    "    \"importance\": rf.feature_importances_\n",
    "}).sort_values(by=\"importance\", ascending=False)\n",
    "print(\"\\nTop 30 features:\\n\", importances.head(30))\n",
    "\n",
    "# -------------------------\n",
    "# 14. Visualizations\n",
    "# -------------------------\n",
    "try:\n",
    "    plt.figure(figsize=(6,6))\n",
    "    plt.scatter(y_test_real, y_test_pred, alpha=0.5)\n",
    "    plt.plot([y_test_real.min(), y_test_real.max()],\n",
    "             [y_test_real.min(), y_test_real.max()], 'r--')\n",
    "    plt.xlabel(\"Actual Total Pollen\")\n",
    "    plt.ylabel(\"Predicted Total Pollen\")\n",
    "    plt.title(\"Actual vs Predicted (Random Forest) â€” Leakage-safe\")\n",
    "    plt.show()\n",
    "\n",
    "    residuals = y_test_real - y_test_pred\n",
    "    plt.figure(figsize=(6,4))\n",
    "    plt.scatter(y_test_pred, residuals, alpha=0.5)\n",
    "    plt.axhline(0, linestyle='--', color='red')\n",
    "    plt.xlabel(\"Predicted Pollen\")\n",
    "    plt.ylabel(\"Residuals\")\n",
    "    plt.title(\"Residuals â€” Leakage-safe RF\")\n",
    "    plt.show()\n",
    "\n",
    "    top_k = min(25, len(importances))\n",
    "    top_feats = importances.head(top_k).sort_values(\"importance\")\n",
    "    plt.figure(figsize=(8, max(4, 0.35*top_k)))\n",
    "    plt.barh(top_feats[\"feature\"], top_feats[\"importance\"])\n",
    "    plt.xlabel(\"Importance\")\n",
    "    plt.title(f\"Top {top_k} Feature Importances â€” Leakage-safe RF\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(12,4))\n",
    "    plt.plot(y_test_real.values, label=\"Actual\")\n",
    "    plt.plot(y_test_pred, label=\"Predicted\")\n",
    "    plt.title(\"Actual vs Predicted Pollen â€” Time Series View (test set)\")\n",
    "    plt.xlabel(\"Test Index (not date)\")\n",
    "    plt.ylabel(\"Pollen Count\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "except Exception as e:\n",
    "    print(\"Plotting failed (non-fatal). Error:\", e)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. LightGBM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T04:06:10.869052Z",
     "iopub.status.busy": "2025-12-10T04:06:10.868872Z",
     "iopub.status.idle": "2025-12-10T04:06:11.866310Z",
     "shell.execute_reply": "2025-12-10T04:06:11.865850Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --------------------------\n",
    "# 1. Prepare dataframe\n",
    "# --------------------------\n",
    "df = merged.copy()\n",
    "\n",
    "# Ensure numeric columns are truly numeric\n",
    "for col in numeric.columns:\n",
    "    df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "\n",
    "# Fill missing numeric values\n",
    "df[numeric.columns] = df[numeric.columns].fillna(0)\n",
    "\n",
    "# Create lag and rolling features\n",
    "df[\"day_of_year\"] = df[\"Date\"].dt.dayofyear\n",
    "df[\"lag1\"] = df[\"Total_Pollen\"].shift(1)\n",
    "df[\"lag2\"] = df[\"Total_Pollen\"].shift(2)\n",
    "df[\"lag3\"] = df[\"Total_Pollen\"].shift(3)\n",
    "df[\"pollen_3day\"] = df[\"Total_Pollen\"].shift(1).rolling(3).mean()\n",
    "df[\"pollen_7day\"] = df[\"Total_Pollen\"].shift(1).rolling(7).mean()\n",
    "\n",
    "# Spike feature\n",
    "df[\"is_spike\"] = ((df[\"Total_Pollen\"].shift(1) - df[\"pollen_3day\"]) > df[\"pollen_3day\"].quantile(0.75)).astype(int)\n",
    "\n",
    "# Drop rows with NaNs in rolling/lag features\n",
    "df = df.dropna(subset=[\"lag1\", \"lag2\", \"lag3\", \"pollen_3day\", \"pollen_7day\"])\n",
    "\n",
    "# --------------------------\n",
    "# 2. Define target & features\n",
    "# --------------------------\n",
    "target = \"Total_Pollen\"\n",
    "remove_cols = [\"Tree\", \"Grass\", \"Weed\", \"Ragweed\", \"Total_Pollen\"]\n",
    "\n",
    "base_features = [c for c in numeric.columns if c not in remove_cols]\n",
    "extra_features = [\"day_of_year\", \"lag1\", \"lag2\", \"lag3\", \"pollen_3day\", \"pollen_7day\", \"is_spike\"]\n",
    "features = base_features + extra_features\n",
    "\n",
    "X = df[features].copy()\n",
    "y = df[target]\n",
    "\n",
    "# --------------------------\n",
    "# 3. Train/test split by year (2024 test)\n",
    "# --------------------------\n",
    "df['Year'] = df['Date'].dt.year\n",
    "\n",
    "train_df = df[df['Year'] < 2023]\n",
    "test_df = df[df['Year'] == 2023]\n",
    "\n",
    "X_train = X.loc[train_df.index]\n",
    "y_train = y.loc[train_df.index]\n",
    "\n",
    "X_test = X.loc[test_df.index]\n",
    "y_test = y.loc[test_df.index]\n",
    "\n",
    "print(\"Train size:\", X_train.shape)\n",
    "print(\"Test size:\", X_test.shape)\n",
    "\n",
    "# --------------------------\n",
    "# 4. LightGBM model\n",
    "# --------------------------\n",
    "lgb_reg = lgb.LGBMRegressor(\n",
    "    n_estimators=1000,\n",
    "    learning_rate=0.05,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "lgb_reg.fit(\n",
    "    X_train, y_train,\n",
    "    eval_set=[(X_test, y_test)],\n",
    "    eval_metric='rmse',\n",
    "    callbacks=[lgb.early_stopping(50), lgb.log_evaluation(50)]\n",
    ")\n",
    "\n",
    "# --------------------------\n",
    "# 5. Predictions\n",
    "# --------------------------\n",
    "y_pred = lgb_reg.predict(X_test)\n",
    "\n",
    "print(\"MAE:\", mean_absolute_error(y_test, y_pred))\n",
    "print(\"RMSE:\", np.sqrt(mean_squared_error(y_test, y_pred)))\n",
    "print(\"RÂ²:\", r2_score(y_test, y_pred))\n",
    "\n",
    "# --------------------------\n",
    "# 6. Feature importance\n",
    "# --------------------------\n",
    "importances = pd.DataFrame({\n",
    "    \"feature\": features,\n",
    "    \"importance\": lgb_reg.feature_importances_\n",
    "}).sort_values(by=\"importance\", ascending=False)\n",
    "\n",
    "print(importances)\n",
    "\n",
    "plt.figure(figsize=(6,6))\n",
    "plt.scatter(y_test, y_pred, alpha=0.5)\n",
    "plt.plot([y_test.min(), y_test.max()],\n",
    "         [y_test.min(), y_test.max()],\n",
    "         'r--')\n",
    "plt.xlabel(\"Actual Pollen\")\n",
    "plt.ylabel(\"Predicted Pollen\")\n",
    "plt.title(\"Actual vs Predicted â€” LightGBM\")\n",
    "plt.show()\n",
    "\n",
    "residuals = y_test - y_pred\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.scatter(y_pred, residuals, alpha=0.5)\n",
    "plt.axhline(0, color='red', linestyle='--')\n",
    "plt.xlabel(\"Predicted Pollen\")\n",
    "plt.ylabel(\"Residual (Actual - Predicted)\")\n",
    "plt.title(\"Residuals â€” LightGBM\")\n",
    "plt.show()\n",
    "\n",
    "top_k = 25\n",
    "top_feats = importances.head(top_k).sort_values(\"importance\")\n",
    "plt.figure(figsize=(8,10))\n",
    "plt.barh(top_feats[\"feature\"], top_feats[\"importance\"])\n",
    "plt.xlabel(\"Importance\")\n",
    "plt.title(f\"Top {top_k} LightGBM Feature Importances\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "lgb.plot_importance(lgb_reg, max_num_features=25, height=0.4, figsize=(8, 10))\n",
    "plt.title(\"LightGBM Feature Importance\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12,4))\n",
    "plt.plot(y_test.values, label=\"Actual\")\n",
    "plt.plot(y_pred, label=\"Predicted\")\n",
    "plt.title(\"Actual vs Predicted â€” Time Series View\")\n",
    "plt.xlabel(\"Test Index\")\n",
    "plt.ylabel(\"Pollen Count\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LightGBM with log transforming pollen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T04:06:11.868880Z",
     "iopub.status.busy": "2025-12-10T04:06:11.868692Z",
     "iopub.status.idle": "2025-12-10T04:06:13.152761Z",
     "shell.execute_reply": "2025-12-10T04:06:13.152369Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --------------------------\n",
    "## 0. Prepare Dataframe (Setup/Assumptions)\n",
    "# --------------------------\n",
    "try:\n",
    "    merged\n",
    "except NameError:\n",
    "    print(\"WARNING: 'merged' dataframe not found. Creating dummy data.\")\n",
    "    dates = pd.to_datetime(pd.date_range(start='2020-01-01', end='2023-12-31', freq='D'))\n",
    "    data = {\n",
    "        \"Date\": dates,\n",
    "        \"Total_Pollen\": np.exp(np.random.normal(3, 1, len(dates))) * (1 + (dates.month % 12)),\n",
    "        \"temperature_2m_mean\": np.random.normal(15, 5, len(dates)),\n",
    "        \"Tree\": np.random.rand(len(dates)),\n",
    "        \"Grass\": np.random.rand(len(dates)),\n",
    "        \"Weed\": np.random.rand(len(dates)),\n",
    "        \"Ragweed\": np.random.rand(len(dates)),\n",
    "        \"Numeric_Column_1\": np.random.rand(len(dates))\n",
    "    }\n",
    "    merged = pd.DataFrame(data)\n",
    "    numeric = [c for c in merged.columns if c != \"Date\"]\n",
    "\n",
    "df = merged.copy()\n",
    "\n",
    "# --------------------------\n",
    "## 1. Clean Column Names & Numeric Conversion\n",
    "# --------------------------\n",
    "df.columns = [c.replace(\" \", \"_\").replace(\"(\", \"\").replace(\")\", \"\") for c in df.columns]\n",
    "numeric_cols = [c.replace(\" \", \"_\").replace(\"(\", \"\").replace(\")\", \"\") for c in numeric if c.replace(\" \", \"_\").replace(\"(\", \"\").replace(\")\", \"\") in df.columns]\n",
    "\n",
    "for col in numeric_cols:\n",
    "    df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "df[numeric_cols] = df[numeric_cols].fillna(0)\n",
    "\n",
    "# --------------------------\n",
    "## 2. Feature Engineering\n",
    "# --------------------------\n",
    "df[\"day_of_year\"] = df[\"Date\"].dt.dayofyear\n",
    "df[\"month\"] = df[\"Date\"].dt.month\n",
    "\n",
    "# Cyclical encoding\n",
    "df[\"day_sin\"] = np.sin(2 * np.pi * df[\"day_of_year\"] / 365)\n",
    "df[\"day_cos\"] = np.cos(2 * np.pi * df[\"day_of_year\"] / 365)\n",
    "df[\"month_sin\"] = np.sin(2 * np.pi * df[\"month\"] / 12)\n",
    "df[\"month_cos\"] = np.cos(2 * np.pi * df[\"month\"] / 12)\n",
    "\n",
    "# Rolling features (shift(1) ensures no look-ahead/leakage)\n",
    "df[\"pollen_2day\"] = df[\"Total_Pollen\"].shift(1).rolling(2).mean()\n",
    "df[\"pollen_3day\"] = df[\"Total_Pollen\"].shift(1).rolling(3).mean()\n",
    "df[\"pollen_7day\"] = df[\"Total_Pollen\"].shift(1).rolling(7).mean()\n",
    "df[\"pollen_14day\"] = df[\"Total_Pollen\"].shift(1).rolling(14).mean()\n",
    "df[\"pollen_30day\"] = df[\"Total_Pollen\"].shift(1).rolling(30).mean()\n",
    "\n",
    "# Lagged features\n",
    "for lag in [1,2,3,4,7,14,21,30]:\n",
    "    df[f\"lag_{lag}\"] = df[\"Total_Pollen\"].shift(lag)\n",
    "\n",
    "# Spike ratio feature\n",
    "df[\"pollen_ratio_1d_7d\"] = df[\"lag_1\"] / df[\"pollen_7day\"]\n",
    "\n",
    "# Drop rows with NaNs from rolling\n",
    "required_cols = [\"pollen_30day\"] # Only need the max window for cleaning\n",
    "df = df.dropna(subset=required_cols).copy()\n",
    "\n",
    "# --------------------------\n",
    "## 3. Train/Validation/Test Split by Year ðŸŽ¯ (Leakage Fix)\n",
    "# --------------------------\n",
    "df[\"Year\"] = df[\"Date\"].dt.year\n",
    "# Train: 2020-2021\n",
    "# Validation: 2022 (Used for early stopping/feature selection)\n",
    "# Test: 2023 (Final, unseen evaluation)\n",
    "train_df = df[df[\"Year\"] < 2022].copy()\n",
    "val_df = df[df[\"Year\"] == 2022].copy()\n",
    "test_df = df[df['Year'] >= 2023]\n",
    "\n",
    "# --------------------------\n",
    "## 4. Target and Spike Feature Transformation\n",
    "# --------------------------\n",
    "# Calculate transformations only on the training set\n",
    "train_cap = train_df[\"Total_Pollen\"].quantile(0.99)\n",
    "spike_threshold = train_df[\"pollen_ratio_1d_7d\"].quantile(0.95)\n",
    "\n",
    "def apply_transformations(data_df, cap_val, spike_thresh):\n",
    "    \"\"\"Applies capping, log transform, and spike feature based on training stats.\"\"\"\n",
    "    data_df[\"Total_Pollen_capped\"] = data_df[\"Total_Pollen\"].clip(upper=cap_val)\n",
    "    data_df[\"log_total_pollen\"] = np.log1p(data_df[\"Total_Pollen_capped\"])\n",
    "    data_df[\"is_spike\"] = (data_df[\"pollen_ratio_1d_7d\"] > spike_thresh).astype(int)\n",
    "    return data_df\n",
    "\n",
    "train_df = apply_transformations(train_df, train_cap, spike_threshold)\n",
    "val_df = apply_transformations(val_df, train_cap, spike_threshold)\n",
    "test_df = apply_transformations(test_df, train_cap, spike_threshold)\n",
    "\n",
    "# --------------------------\n",
    "## 5. Define Features\n",
    "# --------------------------\n",
    "target = \"log_total_pollen\"\n",
    "remove_cols = [\"Tree\",\"Grass\",\"Weed\",\"Ragweed\",\"Total_Pollen\",\"Total_Pollen_capped\",\"log_total_pollen\",\"Date\",\"Year\",\"day_of_year\",\"month\"]\n",
    "\n",
    "base_features = [c for c in numeric_cols if c not in remove_cols]\n",
    "extra_features = [\n",
    "    \"day_sin\",\"day_cos\",\"month_sin\",\"month_cos\",\n",
    "    \"pollen_2day\",\"pollen_3day\",\"pollen_7day\",\"pollen_14day\",\"pollen_30day\",\n",
    "    \"lag_1\",\"lag_2\",\"lag_3\",\"lag_4\",\"lag_7\",\"lag_14\",\"lag_21\",\"lag_30\",\n",
    "    \"pollen_ratio_1d_7d\",\"is_spike\"\n",
    "]\n",
    "features = base_features + extra_features\n",
    "\n",
    "X_train = train_df[features]\n",
    "y_train = train_df[target]\n",
    "X_val = val_df[features]\n",
    "y_val = val_df[target]\n",
    "X_test = test_df[features]\n",
    "y_test = test_df[target]\n",
    "y_test_original = test_df[\"Total_Pollen\"]\n",
    "\n",
    "# Sample weights for spikes\n",
    "train_weights = np.where(train_df[\"is_spike\"]==1, 10, 1)\n",
    "\n",
    "# --------------------------\n",
    "## 6. LightGBM Model & Feature Selection (Leakage Fix)\n",
    "# --------------------------\n",
    "print(\"--- Training Initial Model for Feature Selection ---\")\n",
    "\n",
    "# Use Validation set (X_val, y_val) for Early Stopping\n",
    "lgb_reg = lgb.LGBMRegressor(n_estimators=1000, learning_rate=0.05, random_state=42, n_jobs=-1)\n",
    "lgb_reg.fit(\n",
    "    X_train, y_train,\n",
    "    sample_weight=train_weights,\n",
    "    eval_set=[(X_val, y_val)], # <-- Leakage Fix: Use validation set here\n",
    "    eval_metric='rmse',\n",
    "    callbacks=[lgb.early_stopping(50), lgb.log_evaluation(50)]\n",
    ")\n",
    "\n",
    "# Feature importances\n",
    "importances = pd.DataFrame({\"feature\": features, \"importance\": lgb_reg.feature_importances_}).sort_values(by=\"importance\", ascending=False)\n",
    "selected_features = importances[importances[\"importance\"]>50][\"feature\"].tolist()\n",
    "print(\"\\n--- Selected Features ---\")\n",
    "print(selected_features)\n",
    "\n",
    "X_train_trim = X_train[selected_features]\n",
    "X_val_trim = X_val[selected_features]\n",
    "X_test_trim = X_test[selected_features]\n",
    "train_weights_trim = train_weights[:X_train_trim.shape[0]]\n",
    "\n",
    "print(\"\\n--- Retraining Final Model ---\")\n",
    "# Retrain using trimmed features\n",
    "lgb_reg_trim = lgb.LGBMRegressor(n_estimators=1000, learning_rate=0.05, random_state=42, n_jobs=-1)\n",
    "lgb_reg_trim.fit(\n",
    "    X_train_trim, y_train,\n",
    "    sample_weight=train_weights_trim,\n",
    "    eval_set=[(X_val_trim, y_val)], # <-- Use validation set again\n",
    "    eval_metric='rmse',\n",
    "    callbacks=[lgb.early_stopping(50), lgb.log_evaluation(50)]\n",
    ")\n",
    "\n",
    "# --------------------------\n",
    "## 7. Predictions & Evaluation on Test Set (2023)\n",
    "# --------------------------\n",
    "y_pred_log = lgb_reg_trim.predict(X_test_trim)\n",
    "y_pred = np.expm1(y_pred_log)\n",
    "\n",
    "print(\"\\n--- Model Evaluation (Test Set: 2023) ---\")\n",
    "print(\"MAE:\", mean_absolute_error(y_test_original, y_pred))\n",
    "print(\"RMSE:\", np.sqrt(mean_squared_error(y_test_original, y_pred)))\n",
    "print(\"RÂ²:\", r2_score(y_test_original, y_pred))\n",
    "\n",
    "# Feature importances\n",
    "importances_trim = pd.DataFrame({\"feature\": selected_features, \"importance\": lgb_reg_trim.feature_importances_}).sort_values(by=\"importance\", ascending=False)\n",
    "print(\"\\n--- Feature Importances ---\")\n",
    "print(importances_trim)\n",
    "\n",
    "# --------------------------\n",
    "## 8. Visualizations\n",
    "# --------------------------\n",
    "plt.figure(figsize=(6,6))\n",
    "plt.scatter(y_test_original, y_pred, alpha=0.5)\n",
    "plt.plot([y_test_original.min(), y_test_original.max()],\n",
    "         [y_test_original.min(), y_test_original.max()], 'r--')\n",
    "plt.xlabel(\"Actual Pollen\")\n",
    "plt.ylabel(\"Predicted Pollen\")\n",
    "plt.title(\"Actual vs Predicted (Test Set)\")\n",
    "plt.show()\n",
    "\n",
    "residuals = y_test_original - y_pred\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.scatter(y_pred, residuals, alpha=0.5)\n",
    "plt.axhline(0, color='red', linestyle='--')\n",
    "plt.xlabel(\"Predicted Pollen\")\n",
    "plt.ylabel(\"Residuals\")\n",
    "plt.title(\"Residuals (Test Set)\")\n",
    "plt.show()\n",
    "\n",
    "top_feats = importances_trim.head(15).sort_values(\"importance\")\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.barh(top_feats[\"feature\"], top_feats[\"importance\"])\n",
    "plt.xlabel(\"Importance\")\n",
    "plt.title(\"Top Features\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12,4))\n",
    "plt.plot(test_df[\"Date\"].values, y_test_original.values, label=\"Actual\")\n",
    "plt.plot(test_df[\"Date\"].values, y_pred, label=\"Predicted\")\n",
    "plt.xlabel(\"Date (2023)\")\n",
    "plt.ylabel(\"Pollen Count\")\n",
    "plt.title(\"Actual vs Predicted â€” Time Series (Test Set)\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T04:06:13.154944Z",
     "iopub.status.busy": "2025-12-10T04:06:13.154758Z",
     "iopub.status.idle": "2025-12-10T04:06:15.957962Z",
     "shell.execute_reply": "2025-12-10T04:06:15.957586Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --------------------------\n",
    "## 0. Prepare Dataframe (Setup/Assumptions)\n",
    "# --------------------------\n",
    "try:\n",
    "    merged\n",
    "except NameError:\n",
    "    print(\"WARNING: 'merged' dataframe not found. Creating dummy data.\")\n",
    "    dates = pd.to_datetime(pd.date_range(start='2020-01-01', end='2023-12-31', freq='D'))\n",
    "    \n",
    "    # Create dummy data\n",
    "    len_dates = len(dates)\n",
    "    data = {\n",
    "        \"Date\": dates,\n",
    "        # Total_Pollen is now irrelevant for feature engineering and prediction, but keep it if needed for context\n",
    "        \"Total_Pollen\": np.exp(np.random.normal(5, 1.5, len_dates)) * (1 + (dates.month % 12)),\n",
    "        \"temperature_2m_mean_Â°C\": np.random.normal(15, 5, len_dates),\n",
    "        \"Tree\": np.exp(np.random.normal(4, 2, len_dates)) * (1 + (dates.month % 12)),\n",
    "        \"Grass\": np.exp(np.random.normal(2, 1, len_dates)) * (1 + (dates.month % 12)),\n",
    "        \"Weed\": np.exp(np.random.normal(3, 1.5, len_dates)) * (1 + (dates.month % 12)),\n",
    "        \"Ragweed\": np.random.rand(len_dates) * 50,\n",
    "        \"Numeric_Column_1\": np.random.rand(len_dates) * 10,\n",
    "        \"wind_direction_10m_dominant_Â°\": np.random.randint(0, 360, len_dates),\n",
    "        \"apparent_temperature_mean_Â°C\": np.random.normal(15, 6, len_dates),\n",
    "        \"precipitation_sum_mm\": np.random.rand(len_dates) * 10,\n",
    "        \"O3\": np.random.rand(len_dates) * 50, \"SO2\": np.random.rand(len_dates) * 10, \n",
    "        \"NO2\": np.random.rand(len_dates) * 20, \"PM2.5\": np.random.rand(len_dates) * 30,\n",
    "        \"CO\": np.random.rand(len_dates) * 5, \"AQI\": np.random.randint(0, 100, len_dates)\n",
    "    }\n",
    "    merged = pd.DataFrame(data)\n",
    "    numeric = [c for c in merged.columns if c != \"Date\"]\n",
    "\n",
    "df = merged.copy()\n",
    "\n",
    "# --------------------------\n",
    "## 1. Clean Column Names & Numeric Conversion\n",
    "# --------------------------\n",
    "df.columns = [c.replace(\" \", \"_\").replace(\"(\", \"\").replace(\")\", \"\").replace(\"Â°\", \"\") for c in df.columns]\n",
    "numeric_cols = [c.replace(\" \", \"_\").replace(\"(\", \"\").replace(\")\", \"\").replace(\"Â°\", \"\") for c in numeric if c.replace(\" \", \"_\").replace(\"(\", \"\").replace(\")\", \"\").replace(\"Â°\", \"\") in df.columns]\n",
    "\n",
    "for col in numeric_cols:\n",
    "    df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "df[numeric_cols] = df[numeric_cols].fillna(0)\n",
    "\n",
    "# --------------------------\n",
    "## 2. Feature Engineering (FIXED: Using lagged individual pollen counts instead of Total_Pollen)\n",
    "# --------------------------\n",
    "df[\"day_of_year\"] = df[\"Date\"].dt.dayofyear\n",
    "df[\"month\"] = df[\"Date\"].dt.month\n",
    "\n",
    "# Cyclical encoding\n",
    "df[\"day_sin\"] = np.sin(2 * np.pi * df[\"day_of_year\"] / 365)\n",
    "df[\"day_cos\"] = np.cos(2 * np.pi * df[\"day_of_year\"] / 365)\n",
    "df[\"month_sin\"] = np.sin(2 * np.pi * df[\"month\"] / 12)\n",
    "df[\"month_cos\"] = np.cos(2 * np.pi * df[\"month\"] / 12)\n",
    "\n",
    "# --- Feature Engineering Fix: Create separate lagged/rolling features for each pollen type ---\n",
    "Pollen_Types = [\"Tree\", \"Grass\", \"Weed\"]\n",
    "Lag_Days = [1, 2, 3, 4, 7, 14, 21, 30]\n",
    "\n",
    "engineered_pollen_features = []\n",
    "\n",
    "for pollen_type in Pollen_Types:\n",
    "    # Rolling features (shift(1) ensures no look-ahead/leakage)\n",
    "    df[f\"{pollen_type}_roll_2day\"] = df[pollen_type].shift(1).rolling(2).mean()\n",
    "    df[f\"{pollen_type}_roll_7day\"] = df[pollen_type].shift(1).rolling(7).mean()\n",
    "    df[f\"{pollen_type}_roll_30day\"] = df[pollen_type].shift(1).rolling(30).mean()\n",
    "    engineered_pollen_features.extend([\n",
    "        f\"{pollen_type}_roll_2day\", f\"{pollen_type}_roll_7day\", f\"{pollen_type}_roll_30day\"\n",
    "    ])\n",
    "\n",
    "    # Lagged features\n",
    "    for lag in Lag_Days:\n",
    "        df[f\"{pollen_type}_lag_{lag}\"] = df[pollen_type].shift(lag)\n",
    "        engineered_pollen_features.append(f\"{pollen_type}_lag_{lag}\")\n",
    "\n",
    "    # Spike ratio feature\n",
    "    df[f\"{pollen_type}_ratio_1d_7d\"] = df[f\"{pollen_type}_lag_1\"] / df[f\"{pollen_type}_roll_7day\"]\n",
    "    engineered_pollen_features.append(f\"{pollen_type}_ratio_1d_7d\")\n",
    "# ---------------------------------------------------------------------------------------------\n",
    "\n",
    "# Drop rows with NaNs from rolling (now based on individual pollen types)\n",
    "required_cols = [f\"{pt}_roll_30day\" for pt in Pollen_Types]\n",
    "df = df.dropna(subset=required_cols).copy()\n",
    "\n",
    "# --------------------------\n",
    "## 3. Train/Validation/Test Split by Year ðŸŽ¯\n",
    "# --------------------------\n",
    "df[\"Year\"] = df[\"Date\"].dt.year\n",
    "train_df = df[df[\"Year\"] < 2022].copy()\n",
    "val_df = df[df[\"Year\"] == 2022].copy()\n",
    "test_df = df[df['Year'] >= 2023]\n",
    "\n",
    "# --------------------------\n",
    "## 4. Target Transformation: CONVERT TO MULTIPLE CATEGORICAL LABELS \n",
    "# --------------------------\n",
    "\n",
    "def classify_tree_pollen(count):\n",
    "    \"\"\"0:None, 1:Low (1-14), 2:Moderate (15-89), 3:High (90-1499), 4:Very High (>=1500)\"\"\"\n",
    "    if count < 1: return 0\n",
    "    elif count <= 14: return 1\n",
    "    elif count <= 89: return 2\n",
    "    elif count <= 1499: return 3\n",
    "    else: return 4\n",
    "\n",
    "def classify_weed_pollen(count):\n",
    "    \"\"\"0:None, 1:Low (1-9), 2:Moderate (10-49), 3:High (50-499), 4:Very High (>=500)\"\"\"\n",
    "    if count < 1: return 0\n",
    "    elif count <= 9: return 1\n",
    "    elif count <= 49: return 2\n",
    "    elif count <= 499: return 3\n",
    "    else: return 4\n",
    "\n",
    "def classify_grass_pollen(count):\n",
    "    \"\"\"0:None, 1:Low (1-4), 2:Moderate (5-19), 3:High (20-199), 4:Very High (>=200)\"\"\"\n",
    "    if count < 1: return 0\n",
    "    elif count <= 4: return 1\n",
    "    elif count <= 19: return 2\n",
    "    elif count <= 199: return 3\n",
    "    else: return 4\n",
    "\n",
    "def apply_transformations(data_df):\n",
    "    \"\"\"Applies classification to the Tree, Weed, and Grass columns.\"\"\"\n",
    "    data_df[\"Tree_Level\"] = data_df[\"Tree\"].apply(classify_tree_pollen).astype('category')\n",
    "    data_df[\"Weed_Level\"] = data_df[\"Weed\"].apply(classify_weed_pollen).astype('category')\n",
    "    data_df[\"Grass_Level\"] = data_df[\"Grass\"].apply(classify_grass_pollen).astype('category')\n",
    "    return data_df\n",
    "\n",
    "train_df = apply_transformations(train_df)\n",
    "val_df = apply_transformations(val_df)\n",
    "test_df = apply_transformations(test_df)\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "## 5. Define Features (FIXED: Updated features list)\n",
    "# --------------------------\n",
    "Target_Levels = [f\"{pt}_Level\" for pt in Pollen_Types]\n",
    "\n",
    "# Features: exclude the original continuous pollen counts and old target columns\n",
    "remove_cols = [\"Tree\",\"Grass\",\"Weed\",\"Ragweed\",\"Total_Pollen\",\"Date\",\"Year\",\"day_of_year\",\"month\"]\n",
    "remove_cols.extend(Target_Levels)\n",
    "\n",
    "# Remove the 'pollen' column names from the numeric columns before defining base features\n",
    "numeric_no_pollen_cols = [c for c in numeric_cols if c not in [\"Tree\",\"Grass\",\"Weed\",\"Ragweed\",\"Total_Pollen\"]]\n",
    "\n",
    "base_features = [c for c in numeric_no_pollen_cols if c not in remove_cols]\n",
    "extra_features = [\n",
    "    \"day_sin\",\"day_cos\",\"month_sin\",\"month_cos\",\n",
    "]\n",
    "\n",
    "# Add the newly engineered, type-specific lagged/rolling features\n",
    "extra_features.extend(engineered_pollen_features) \n",
    "\n",
    "features = base_features + extra_features\n",
    "\n",
    "X_train_full = train_df[features]\n",
    "X_val_full = val_df[features]\n",
    "X_test_full = test_df[features]\n",
    "\n",
    "# --------------------------\n",
    "## 6. Train Three Separate Classification Models \n",
    "# --------------------------\n",
    "Trained_Models = {}\n",
    "Test_Predictions = {}\n",
    "\n",
    "print(\"--- Training Three Independent Models (Tree, Weed, Grass) ---\")\n",
    "\n",
    "for target in Target_Levels:\n",
    "    print(f\"\\nTraining Model for: **{target}**\")\n",
    "\n",
    "    y_train_target = train_df[target]\n",
    "    y_val_target = val_df[target]\n",
    "\n",
    "    # Calculate class weights based on the current target's training data\n",
    "    class_counts = y_train_target.value_counts().sort_index()\n",
    "    total_samples = len(y_train_target)\n",
    "    num_classes = len(class_counts)\n",
    "    \n",
    "    # Ensure all classes from 0 to max_level are accounted for (for robust classification)\n",
    "    max_level = max(class_counts.index)\n",
    "    \n",
    "    class_weights = {}\n",
    "    for i in range(max_level + 1):\n",
    "        count = class_counts.get(i, 1e-6) # Use a tiny number if class is missing to avoid division by zero\n",
    "        class_weights[i] = total_samples / (num_classes * count)\n",
    "    \n",
    "    print(f\"  Class Counts: {class_counts.to_dict()}\")\n",
    "    print(f\"  Class Weights: {{0 (None), 1 (Low), 2 (Mod), 3 (High), 4 (V.High)}} = {class_weights}\")\n",
    "    \n",
    "    # Initialize the classifier\n",
    "    lgb_clf = lgb.LGBMClassifier(\n",
    "        objective='multiclass',\n",
    "        num_class=max_level + 1, # e.g., 5 classes (0 to 4)\n",
    "        n_estimators=1000,\n",
    "        learning_rate=0.05,\n",
    "        random_state=42,\n",
    "        n_jobs=-1,\n",
    "        class_weight=class_weights # Apply weights for imbalance\n",
    "    )\n",
    "    \n",
    "    # Train the model\n",
    "    lgb_clf.fit(\n",
    "        X_train_full, y_train_target,\n",
    "        eval_set=[(X_val_full, y_val_target)],\n",
    "        eval_metric='multi_logloss',\n",
    "        callbacks=[lgb.early_stopping(50), lgb.log_evaluation(50)]\n",
    "    )\n",
    "    \n",
    "    # Store the trained model\n",
    "    Trained_Models[target] = lgb_clf\n",
    "    \n",
    "    # Generate predictions on the test set\n",
    "    y_pred = lgb_clf.predict(X_test_full)\n",
    "    Test_Predictions[target] = y_pred\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "## 7. Predictions & Evaluation on Test Set (2023) \n",
    "# --------------------------\n",
    "target_names_4 = ['None (0)', 'Low (1)', 'Moderate (2)', 'High (3)', 'Very High (4)']\n",
    "full_labels = [0, 1, 2, 3, 4] \n",
    "\n",
    "for target in Target_Levels:\n",
    "    print(f\"\\n--- Model Evaluation (Test Set: 2023) for **{target}** ---\")\n",
    "    \n",
    "    y_test_target = test_df[target]\n",
    "    y_pred_class = Test_Predictions[target]\n",
    "    \n",
    "    print(\"Accuracy:\", accuracy_score(y_test_target, y_pred_class))\n",
    "    \n",
    "    # Classification Report \n",
    "    print(\"\\nClassification Report:\\n\", classification_report(\n",
    "        y_test_target, \n",
    "        y_pred_class, \n",
    "        target_names=target_names_4, \n",
    "        labels=full_labels, \n",
    "        zero_division=0\n",
    "    ))\n",
    "    \n",
    "    # Confusion Matrix \n",
    "    conf_matrix = confusion_matrix(y_test_target, y_pred_class, labels=full_labels)\n",
    "    print(\"\\nConfusion Matrix:\\n\", conf_matrix)\n",
    "\n",
    "# --------------------------\n",
    "## 8. Visualizations \n",
    "# --------------------------\n",
    "\n",
    "# Display feature importances for one model (e.g., Tree_Level)\n",
    "target_to_plot = \"Tree_Level\"\n",
    "lgb_clf_plot = Trained_Models[target_to_plot]\n",
    "\n",
    "importances_trim = pd.DataFrame({\"feature\": features, \"importance\": lgb_clf_plot.feature_importances_}).sort_values(by=\"importance\", ascending=False)\n",
    "print(f\"\\n--- Feature Importances for {target_to_plot} Model ---\")\n",
    "print(importances_trim.head(15))\n",
    "\n",
    "top_feats = importances_trim.head(15).sort_values(\"importance\")\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.barh(top_feats[\"feature\"], top_feats[\"importance\"])\n",
    "plt.xlabel(\"Importance\")\n",
    "plt.title(f\"Top Features for {target_to_plot} Classification (Leakage Fixed)\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Time series plot for one classification result (e.g., Tree_Level)\n",
    "y_test_plot = test_df[target_to_plot].values\n",
    "y_pred_plot = Test_Predictions[target_to_plot]\n",
    "\n",
    "plt.figure(figsize=(12,4))\n",
    "plt.plot(test_df[\"Date\"].values, y_test_plot, label=\"Actual Class\", marker='.', linestyle='', alpha=0.7)\n",
    "plt.plot(test_df[\"Date\"].values, y_pred_plot, label=\"Predicted Class\", marker='x', linestyle='', alpha=0.7)\n",
    "plt.xlabel(\"Date (2023)\")\n",
    "plt.ylabel(\"Pollen Level (0-4)\")\n",
    "plt.title(f\"Actual vs Predicted {target_to_plot} â€” Time Series (Test Set)\")\n",
    "plt.yticks(range(len(target_names_4)), target_names_4)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. XGBoost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T04:06:15.960370Z",
     "iopub.status.busy": "2025-12-10T04:06:15.960161Z",
     "iopub.status.idle": "2025-12-10T04:06:16.649916Z",
     "shell.execute_reply": "2025-12-10T04:06:16.649446Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --------------------------\n",
    "# 1. Prepare dataframe & Time-Series Split (Split FIRST for safe calculation)\n",
    "# --------------------------\n",
    "df = merged.copy()\n",
    "\n",
    "# ----------------------------------\n",
    "# Normalize dataframe column names\n",
    "# ----------------------------------\n",
    "df.columns = (\n",
    "    df.columns\n",
    "    .str.replace(\" \", \"_\", regex=False)\n",
    "    .str.replace(\"(\", \"\", regex=False)\n",
    "    .str.replace(\")\", \"\", regex=False)\n",
    "    .str.replace(\"/\", \"_\", regex=False)\n",
    ")\n",
    "# Normalize numeric columns list for consistency\n",
    "numeric_cols = [\n",
    "    c.replace(\" \", \"_\").replace(\"(\", \"\").replace(\")\", \"\").replace(\"/\", \"_\")\n",
    "    for c in numeric.columns if c in df.columns\n",
    "]\n",
    "\n",
    "\n",
    "# Ensure numeric columns and fill NaNs (Fillna must use ffill or a model trained only on past data)\n",
    "for col in numeric_cols:\n",
    "    df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "\n",
    "# Fill missing numeric values with 0.\n",
    "df[numeric_cols] = df[numeric_cols].fillna(0)\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# 2. Define Features and Perform Train/Test Split\n",
    "#    Split is done here to ensure threshold calculations are safe.\n",
    "# --------------------------\n",
    "target = \"Total_Pollen\"\n",
    "\n",
    "# Split data chronologically (CORRECT METHOD)\n",
    "df['Year'] = df['Date'].dt.year\n",
    "train_idx = df[df['Year'] < 2023].index\n",
    "test_idx = df[df['Year'] == 2023].index\n",
    "\n",
    "train_df = df.loc[train_idx].copy()\n",
    "test_df = df.loc[test_idx].copy()\n",
    "full_df = df.copy() # Keep a copy for calculating lags/rolling across entire series\n",
    "\n",
    "# --------------------------\n",
    "# 3. Feature Engineering (Apply to Full Data, but only use safe methods: ffill, shift, rolling)\n",
    "# --------------------------\n",
    "\n",
    "# Base lags + rolling (SAFE: Uses .shift(1) and ffill is causal)\n",
    "full_df[\"day_of_year\"] = full_df[\"Date\"].dt.dayofyear\n",
    "full_df[\"lag1\"] = full_df[target].shift(1).ffill()\n",
    "full_df[\"lag2\"] = full_df[target].shift(2).ffill()\n",
    "full_df[\"lag3\"] = full_df[target].shift(3).ffill()\n",
    "\n",
    "# Shift(1) before rolling ensures target leakage is avoided\n",
    "full_df[\"pollen_3day\"] = full_df[target].shift(1).rolling(3).mean().ffill()\n",
    "full_df[\"pollen_7day\"] = full_df[target].shift(1).rolling(7).mean().ffill()\n",
    "\n",
    "\n",
    "# Spike indicator (FIXED: Uses only lagged features, avoiding target leakage)\n",
    "# Using lag1 > rolling mean as a proxy for a sudden spike\n",
    "full_df[\"is_spike\"] = (full_df[\"lag1\"] > full_df[\"pollen_3day\"]).astype(int)\n",
    "\n",
    "\n",
    "# Seasonal sin/cos\n",
    "full_df[\"sin_day\"] = np.sin(2 * np.pi * full_df[\"day_of_year\"] / 365)\n",
    "full_df[\"cos_day\"] = np.cos(2 * np.pi * full_df[\"day_of_year\"] / 365)\n",
    "\n",
    "# Weighted rolling (3-day)\n",
    "weights = np.array([0.1, 0.3, 0.6])\n",
    "rolling_cols = [\n",
    "    'temperature_2m_mean_Â°C', 'apparent_temperature_mean_Â°C',\n",
    "    'PM2.5', 'O3', 'CO', 'NO2', 'SO2',\n",
    "    'AQI_PM2.5', 'AQI_O3', 'AQI_CO', 'AQI_NO2', 'AQI_SO2', 'AQI'\n",
    "]\n",
    "\n",
    "# Weighted rolling (FIXED: Only uses lagged values of weather/pollutants, t-1, t-2, t-3)\n",
    "for col in rolling_cols:\n",
    "    if col in full_df.columns:\n",
    "        full_df[f'{col}_weighted3'] = (\n",
    "            full_df[col].shift(3).ffill() * weights[0] +\n",
    "            full_df[col].shift(2).ffill() * weights[1] +\n",
    "            full_df[col].shift(1).ffill() * weights[2]\n",
    "        )\n",
    "\n",
    "# Lag features (FIXED: Uses ffill instead of bfill for causal imputation)\n",
    "for col in rolling_cols:\n",
    "    if col in full_df.columns:\n",
    "        for lag in range(1, 4):\n",
    "            full_df[f\"{col}_lag{lag}\"] = full_df[col].shift(lag).ffill()\n",
    "\n",
    "# Interaction features (SAFE: Uses current day's features which are available on the day of prediction)\n",
    "interaction_pairs = [\n",
    "    ('temperature_2m_mean_Â°C', 'AQI'),\n",
    "    ('apparent_temperature_mean_Â°C', 'PM2.5'),\n",
    "    ('wind_speed_10m_max_km_h', 'O3'),\n",
    "]\n",
    "\n",
    "for col1, col2 in interaction_pairs:\n",
    "    if col1 in full_df.columns and col2 in full_df.columns:\n",
    "        full_df[f'{col1}_x_{col2}'] = full_df[col1] * full_df[col2]\n",
    "\n",
    "# Drop rows with NaNs remaining from lag/rolling features (first few days)\n",
    "full_df = full_df.dropna(subset=[\"lag1\", \"lag2\", \"lag3\", \"pollen_3day\", \"pollen_7day\"])\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# 4. Final Data Split & Feature Set Definition\n",
    "# --------------------------\n",
    "remove_cols = [\"Tree\", \"Grass\", \"Weed\", \"Ragweed\", \"Total_Pollen\", \"Year\", \"Date\"]\n",
    "\n",
    "base_features = [c for c in numeric_cols if c not in remove_cols]\n",
    "\n",
    "extra_features = [\n",
    "    \"lag1\", \"lag2\", \"lag3\",\n",
    "    \"pollen_3day\", \"pollen_7day\",\n",
    "    \"is_spike\", \"sin_day\", \"cos_day\"\n",
    "]\n",
    "\n",
    "weighted_features = [f\"{col}_weighted3\" for col in rolling_cols if f\"{col}_weighted3\" in full_df.columns]\n",
    "lag_features = [f\"{col}_lag{lag}\" for col in rolling_cols for lag in range(1, 4)]\n",
    "interaction_features = [f\"{a}_x_{b}\" for a, b in interaction_pairs]\n",
    "\n",
    "features = base_features + extra_features + weighted_features + lag_features + interaction_features\n",
    "\n",
    "# Re-filter features that actually exist in the final df\n",
    "features = [f for f in features if f in full_df.columns]\n",
    "\n",
    "# Apply the chronological split indices to the final feature set\n",
    "X_train = full_df.loc[full_df['Year'] < 2023, features]\n",
    "y_train = full_df.loc[full_df['Year'] < 2023, target]\n",
    "\n",
    "X_test = full_df.loc[full_df['Year'] == 2023, features]\n",
    "y_test = full_df.loc[full_df['Year'] == 2023, target]\n",
    "\n",
    "\n",
    "print(\"Train size:\", X_train.shape)\n",
    "print(\"Test size:\", X_test.shape)\n",
    "\n",
    "# --------------------------\n",
    "# 5. Convert to DMatrix\n",
    "# --------------------------\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "dtest = xgb.DMatrix(X_test, label=y_test)\n",
    "\n",
    "# --------------------------\n",
    "# 6. XGBoost parameters\n",
    "# --------------------------\n",
    "params = {\n",
    "    'objective': 'reg:squarederror',\n",
    "    'learning_rate': 0.05,\n",
    "    'max_depth': 6,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'tree_method': 'hist',\n",
    "    'eval_metric': 'rmse',\n",
    "    'seed': 42\n",
    "}\n",
    "\n",
    "# --------------------------\n",
    "# 7. Train with early stopping\n",
    "# --------------------------\n",
    "evals = [(dtrain, 'train'), (dtest, 'eval')]\n",
    "xgb_model = xgb.train(\n",
    "    params=params,\n",
    "    dtrain=dtrain,\n",
    "    num_boost_round=1200,\n",
    "    evals=evals,\n",
    "    early_stopping_rounds=50,\n",
    "    verbose_eval=50\n",
    ")\n",
    "\n",
    "# --------------------------\n",
    "# 8. Predictions and Evaluation (Rest of the code remains the same)\n",
    "# --------------------------\n",
    "y_pred = xgb_model.predict(dtest)\n",
    "\n",
    "print(\"\\n--- Model Performance ---\")\n",
    "print(\"MAE:\", mean_absolute_error(y_test, y_pred))\n",
    "print(\"RMSE:\", np.sqrt(mean_squared_error(y_test, y_pred)))\n",
    "print(\"RÂ²:\", r2_score(y_test, y_pred))\n",
    "\n",
    "# --------------------------\n",
    "# 9. Feature importance\n",
    "# --------------------------\n",
    "importance = xgb_model.get_score(importance_type='gain')\n",
    "importance_df = pd.DataFrame({\n",
    "    'feature': list(importance.keys()),\n",
    "    'importance': list(importance.values())\n",
    "}).sort_values(by='importance', ascending=False)\n",
    "\n",
    "print(\"\\n--- Feature Importance ---\")\n",
    "print(importance_df.head(10))\n",
    "\n",
    "# --------------------------\n",
    "# 10. Visualizations\n",
    "# --------------------------\n",
    "\n",
    "# Time series view of predictions vs actual\n",
    "plt.figure(figsize=(12,4))\n",
    "plt.plot(y_test.values, label=\"Actual\")\n",
    "plt.plot(y_pred, label=\"Predicted\")\n",
    "plt.title(\"Actual vs Predicted Pollen â€” Time Series (2023)\")\n",
    "plt.xlabel(\"Test Index\")\n",
    "plt.ylabel(\"Pollen Count\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Scatter plot\n",
    "plt.figure(figsize=(6,6))\n",
    "plt.scatter(y_test, y_pred, alpha=0.5)\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')\n",
    "plt.xlabel(\"Actual Total Pollen\")\n",
    "plt.ylabel(\"Predicted Total Pollen\")\n",
    "plt.title(\"Actual vs Predicted â€” XGBoost\")\n",
    "plt.show()\n",
    "\n",
    "# Feature importance plot\n",
    "top_k = 25\n",
    "top_feats = importance_df.head(top_k).sort_values(\"importance\")\n",
    "plt.figure(figsize=(8,10))\n",
    "plt.barh(top_feats[\"feature\"], top_feats[\"importance\"])\n",
    "plt.xlabel(\"Importance (Gain)\")\n",
    "plt.title(f\"Top {top_k} XGBoost Feature Importances\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# two stage model with same day weather data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T04:06:16.652324Z",
     "iopub.status.busy": "2025-12-10T04:06:16.652183Z",
     "iopub.status.idle": "2025-12-10T04:06:20.604796Z",
     "shell.execute_reply": "2025-12-10T04:06:20.604271Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# --------------------------\n",
    "## 0. Prepare Dataframe\n",
    "# --------------------------\n",
    "def standardize_columns(df):\n",
    "    cols = df.columns\n",
    "    cols = (\n",
    "        cols.str.strip()\n",
    "        .str.lower()\n",
    "        .str.replace('Â°c','', regex=False)\n",
    "        .str.replace('Â°','', regex=False)\n",
    "        .str.replace('mm','', regex=False)\n",
    "        .str.replace('/','_', regex=False)\n",
    "        .str.replace(' ', '_', regex=False)\n",
    "        .str.replace('(', '', regex=False)\n",
    "        .str.replace(')', '', regex=False)\n",
    "        .str.replace('.', '_', regex=False)\n",
    "    )\n",
    "    df.columns = cols\n",
    "    return df\n",
    "\n",
    "# Dummy dataset if needed\n",
    "try:\n",
    "    merged\n",
    "except NameError:\n",
    "    dates = pd.to_datetime(pd.date_range(start='2020-01-01', end='2023-12-31', freq='D'))\n",
    "    data = {\n",
    "        \"Date\": dates,\n",
    "        \"Total_Pollen\": np.exp(np.random.normal(3, 1, len(dates))) * (1 + (dates.month % 12)),\n",
    "        \"temperature_2m_mean\": np.random.normal(15, 5, len(dates)),\n",
    "        \"relativehumidity_2m_mean\": np.random.uniform(30, 90, len(dates)),\n",
    "        \"wind_speed_10m_max\": np.random.uniform(0, 20, len(dates)),\n",
    "        \"precipitation_sum\": np.random.uniform(0, 10, len(dates)),\n",
    "        \"Numeric_Column_1\": np.random.rand(len(dates))\n",
    "    }\n",
    "    merged = pd.DataFrame(data)\n",
    "\n",
    "df = standardize_columns(merged)\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "numeric_cols = df.select_dtypes(include=['number']).columns.tolist()\n",
    "\n",
    "# --------------------------\n",
    "## 1. Feature Engineering (Past pollen + forecasted weather)\n",
    "# --------------------------\n",
    "def make_time_features(df):\n",
    "    df['day_of_year'] = df['date'].dt.dayofyear\n",
    "    df['month'] = df['date'].dt.month\n",
    "    df['day_sin'] = np.sin(2 * np.pi * df['day_of_year'] / 365.25)\n",
    "    df['day_cos'] = np.cos(2 * np.pi * df['day_of_year'] / 365.25)\n",
    "    df['month_sin'] = np.sin(2 * np.pi * df['month'] / 12)\n",
    "    df['month_cos'] = np.cos(2 * np.pi * df['month'] / 12)\n",
    "    return df\n",
    "\n",
    "def make_lags_rolls(df, target_col='total_pollen', lags=[1,2,3,4,7,14,21,30], rolls=[2,3,7,14,30]):\n",
    "    \"\"\"Lagged/rolling pollen features for next-day prediction.\"\"\"\n",
    "    for lag in lags:\n",
    "        df[f'lag_{lag}'] = df[target_col].shift(lag)\n",
    "    for r in rolls:\n",
    "        df[f'pollen_{r}day'] = df[target_col].shift(1).rolling(window=r, min_periods=1).mean()\n",
    "    df['pollen_ratio_1d_7d'] = df['lag_1'] / (df['pollen_7day'] + 1e-8)\n",
    "    return df\n",
    "\n",
    "def make_weather_forecast_features(df, weather_cols):\n",
    "    \"\"\"Shift weather by 1 to simulate forecast (predicting next day).\"\"\"\n",
    "    for col in weather_cols:\n",
    "        if col in df.columns:\n",
    "            df[f'{col}_forecast'] = df[col].shift(1)\n",
    "    return df\n",
    "\n",
    "df = make_time_features(df)\n",
    "df = make_lags_rolls(df)\n",
    "weather_cols = ['temperature_2m_mean', 'relativehumidity_2m_mean', 'wind_speed_10m_max', 'precipitation_sum']\n",
    "df = make_weather_forecast_features(df, weather_cols)\n",
    "\n",
    "# Fill missing values\n",
    "df[numeric_cols] = df[numeric_cols].fillna(method='ffill').fillna(0)\n",
    "df = df.dropna(subset=['pollen_30day']).copy()\n",
    "\n",
    "# --------------------------\n",
    "## 2. Train/Val/Test Split\n",
    "# --------------------------\n",
    "df['year'] = df['date'].dt.year\n",
    "train_df = df[df['year'] < 2022].copy()\n",
    "val_df   = df[df['year'] == 2022].copy()\n",
    "test_df  = df[df['year'] >= 2023].copy()\n",
    "\n",
    "# --------------------------\n",
    "## 3. Spike Labeling & Target Transform\n",
    "# --------------------------\n",
    "def asinh_transform(x): return np.arcsinh(x)\n",
    "def asinh_inverse(x): return np.sinh(x)\n",
    "\n",
    "base_mean = train_df['pollen_ratio_1d_7d'].mean()\n",
    "base_std = train_df['pollen_ratio_1d_7d'].std(ddof=0)\n",
    "z_thresh = 1.64\n",
    "\n",
    "for df_ in [train_df, val_df, test_df]:\n",
    "    df_['spike_z'] = (df_['pollen_ratio_1d_7d'] - base_mean) / (base_std + 1e-8)\n",
    "    df_['is_spike'] = (df_['spike_z'] > z_thresh).astype(int)\n",
    "    df_['target_asinh'] = asinh_transform(df_['total_pollen'])\n",
    "\n",
    "# --------------------------\n",
    "## 4. Features\n",
    "# --------------------------\n",
    "exclude_cols = {'date','year','total_pollen','weed','tree','grass','target_asinh','is_spike','spike_z'}\n",
    "all_features = [c for c in train_df.columns if c not in exclude_cols and train_df[c].dtype in [np.float64, np.int64]]\n",
    "\n",
    "all_features = [c for c in all_features if train_df[c].nunique() > 1]\n",
    "\n",
    "X_train = train_df[all_features]; y_train = train_df['target_asinh']\n",
    "X_val = val_df[all_features]; y_val = val_df['target_asinh']\n",
    "X_test = test_df[all_features]; y_test_original = test_df['total_pollen']\n",
    "\n",
    "# --------------------------\n",
    "## 5. Train Classifier\n",
    "# --------------------------\n",
    "clf_params = {\"n_estimators\":1000,\"learning_rate\":0.05,\"num_leaves\":31,\"min_data_in_leaf\":20,\n",
    "              \"feature_fraction\":0.8,\"bagging_fraction\":0.8,\"bagging_freq\":5,\"random_state\":42,\"n_jobs\":-1}\n",
    "clf = lgb.LGBMClassifier(**clf_params)\n",
    "clf.fit(X_train, train_df['is_spike'])\n",
    "val_spike_prob = clf.predict_proba(X_val)[:,1]\n",
    "test_spike_prob = clf.predict_proba(X_test)[:,1]\n",
    "print(f\"Classifier val AUC: {roc_auc_score(val_df['is_spike'], val_spike_prob):.3f}\")\n",
    "\n",
    "# --------------------------\n",
    "## 6. Train Regressors\n",
    "# --------------------------\n",
    "reg_params = {\"n_estimators\":3000,\"learning_rate\":0.03,\"num_leaves\":64,\"min_data_in_leaf\":40,\n",
    "              \"lambda_l1\":1.0,\"lambda_l2\":1.0,\"feature_fraction\":0.8,\"bagging_fraction\":0.8,\n",
    "              \"bagging_freq\":5,\"random_state\":42,\"n_jobs\":-1}\n",
    "\n",
    "reg_normal = lgb.LGBMRegressor(**reg_params)\n",
    "reg_normal.fit(X_train[train_df['is_spike']==0], y_train[train_df['is_spike']==0])\n",
    "\n",
    "reg_spike = lgb.LGBMRegressor(**reg_params)\n",
    "reg_spike.fit(X_train[train_df['is_spike']==1], y_train[train_df['is_spike']==1])\n",
    "\n",
    "# --------------------------\n",
    "## 7. Feature Importance\n",
    "# --------------------------\n",
    "def print_feature_importance(model, model_name):\n",
    "    importance = pd.DataFrame({\n",
    "        'feature': model.feature_name_,\n",
    "        'importance': model.feature_importances_\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    print(f\"\\nFeature Importance for {model_name}:\\n\", importance)\n",
    "    return importance\n",
    "\n",
    "clf_importance = print_feature_importance(clf, \"Spike Classifier\")\n",
    "reg_normal_importance = print_feature_importance(reg_normal, \"Normal Regressor\")\n",
    "reg_spike_importance = print_feature_importance(reg_spike, \"Spike Regressor\")\n",
    "\n",
    "# --------------------------\n",
    "## 8. Predictions\n",
    "# --------------------------\n",
    "pred_normal_test = reg_normal.predict(X_test)\n",
    "pred_spike_test = reg_spike.predict(X_test)\n",
    "combined_test_trans = test_spike_prob * pred_spike_test + (1 - test_spike_prob) * pred_normal_test\n",
    "y_pred_test = asinh_inverse(combined_test_trans)\n",
    "\n",
    "mae = mean_absolute_error(y_test_original, y_pred_test)\n",
    "rmse = np.sqrt(mean_squared_error(y_test_original, y_pred_test))\n",
    "r2 = r2_score(y_test_original, y_pred_test)\n",
    "print(f\"\\nTest MAE: {mae:.3f}, RMSE: {rmse:.3f}, R^2: {r2:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# above with no same-day weather data \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-10T04:06:20.607567Z",
     "iopub.status.busy": "2025-12-10T04:06:20.607130Z",
     "iopub.status.idle": "2025-12-10T04:06:21.567157Z",
     "shell.execute_reply": "2025-12-10T04:06:21.566776Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "from lightgbm import early_stopping\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, roc_auc_score\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# --------------------------\n",
    "## 0. Standardize Columns\n",
    "# --------------------------\n",
    "def standardize_columns(df):\n",
    "    df = df.copy()\n",
    "    cols = df.columns\n",
    "    cols = (\n",
    "        cols.str.strip()\n",
    "        .str.lower()\n",
    "        .str.replace('Â°c','', regex=False)\n",
    "        .str.replace('Â°','', regex=False)\n",
    "        .str.replace('mm','', regex=False)\n",
    "        .str.replace('/','_', regex=False)\n",
    "        .str.replace(' ', '_', regex=False)\n",
    "        .str.replace('(', '', regex=False)\n",
    "        .str.replace(')', '', regex=False)\n",
    "        .str.replace('.', '_', regex=False)\n",
    "    )\n",
    "    df.columns = cols\n",
    "    return df\n",
    "\n",
    "# --------------------------\n",
    "## 0b. Dummy Data if Needed\n",
    "# --------------------------\n",
    "try:\n",
    "    merged\n",
    "except NameError:\n",
    "    dates = pd.date_range('2020-01-01','2023-12-31')\n",
    "    merged = pd.DataFrame({\n",
    "        \"Date\": dates,\n",
    "        \"Total_Pollen\": np.exp(np.random.normal(3, 1, len(dates))) * (1 + (dates.month % 12)),\n",
    "        \"temperature_2m_mean\": np.random.normal(15,5,len(dates)),\n",
    "        \"relativehumidity_2m_mean\": np.random.uniform(30,90,len(dates)),\n",
    "        \"wind_speed_10m_max\": np.random.uniform(0,20,len(dates)),\n",
    "        \"precipitation_sum\": np.random.uniform(0,10,len(dates)),\n",
    "        \"Numeric_Column_1\": np.random.rand(len(dates))\n",
    "    })\n",
    "\n",
    "df = standardize_columns(merged)\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "numeric_cols = df.select_dtypes(include=['number']).columns.tolist()\n",
    "\n",
    "# --------------------------\n",
    "## 1. Feature Engineering\n",
    "# --------------------------\n",
    "def make_time_features(df):\n",
    "    df['day_of_year'] = df['date'].dt.dayofyear\n",
    "    df['month'] = df['date'].dt.month\n",
    "    df['weekday'] = df['date'].dt.weekday\n",
    "    df['day_sin'] = np.sin(2*np.pi*df['day_of_year']/365.25)\n",
    "    df['day_cos'] = np.cos(2*np.pi*df['day_of_year']/365.25)\n",
    "    df['month_sin'] = np.sin(2*np.pi*df['month']/12)\n",
    "    df['month_cos'] = np.cos(2*np.pi*df['month']/12)\n",
    "    df['weekday_sin'] = np.sin(2*np.pi*df['weekday']/7)\n",
    "    df['weekday_cos'] = np.cos(2*np.pi*df['weekday']/7)\n",
    "    return df\n",
    "\n",
    "def make_lags_rolls(df, target_col='total_pollen', lags=[1,2,3,4,7,14,21,30,365], rolls=[2,3,7,14,30]):\n",
    "    for lag in lags:\n",
    "        df[f'lag_{lag}'] = df[target_col].shift(lag)\n",
    "\n",
    "        if lag > 1:\n",
    "            df[f'lag_{lag}_pct_fixed'] = (df['lag_1'] - df[f'lag_{lag}']) / df[f'lag_{lag}']\n",
    "        else:\n",
    "            df[f'lag_{lag}_pct_fixed'] = np.nan\n",
    "\n",
    "        \n",
    "    for r in rolls:\n",
    "        df[f'{target_col}_{r}day'] = df[target_col].shift(1).rolling(r, min_periods=1).mean()\n",
    "    df['pollen_ratio_1d_7d'] = df['lag_1'] / (df['total_pollen_7day'] + 1e-8)\n",
    "    return df\n",
    "\n",
    "def make_weather_features_yesterday(df, weather_cols, rolls=[3,7]):\n",
    "    \"\"\"\n",
    "    Weather features only use data up to yesterday.\n",
    "    \"\"\"\n",
    "    for col in weather_cols:\n",
    "        if col in df.columns:\n",
    "            df[f'{col}_yesterday'] = df[col].shift(1)\n",
    "            for r in rolls:\n",
    "                df[f'{col}_yesterday_{r}day'] = df[col].shift(1).rolling(r, min_periods=1).mean()\n",
    "    return df\n",
    "\n",
    "# Apply feature engineering\n",
    "df = make_time_features(df)\n",
    "df = make_lags_rolls(df)\n",
    "weather_cols = ['temperature_2m_mean','relativehumidity_2m_mean','wind_speed_10m_max','precipitation_sum']\n",
    "df = make_weather_features_yesterday(df, weather_cols)\n",
    "\n",
    "# --------------------------\n",
    "## 1b. Fill Missing Values\n",
    "# --------------------------\n",
    "df[numeric_cols] = df[numeric_cols].interpolate().fillna(0)\n",
    "df = df.dropna(subset=['total_pollen_30day'])\n",
    "\n",
    "# --------------------------\n",
    "## 2. Train/Val/Test Split\n",
    "# --------------------------\n",
    "df['year'] = df['date'].dt.year\n",
    "train_df = df[df['year']<2022].copy()\n",
    "val_df = df[df['year']==2022].copy()\n",
    "test_df = df[df['year']>=2023].copy()\n",
    "\n",
    "# --------------------------\n",
    "## 3. Spike Labeling & Target Transform\n",
    "# --------------------------\n",
    "def asinh_transform(x): return np.arcsinh(x)\n",
    "def asinh_inverse(x): return np.sinh(x)\n",
    "\n",
    "base_mean = train_df['pollen_ratio_1d_7d'].mean()\n",
    "base_std = train_df['pollen_ratio_1d_7d'].std(ddof=0)\n",
    "z_thresh = 1.64\n",
    "\n",
    "for df_ in [train_df,val_df,test_df]:\n",
    "    df_['spike_z'] = (df_['pollen_ratio_1d_7d'] - base_mean)/(base_std+1e-8)\n",
    "    df_['is_spike'] = (df_['spike_z']>z_thresh).astype(int)\n",
    "    df_['target_asinh'] = asinh_transform(df_['total_pollen'])\n",
    "\n",
    "# --------------------------\n",
    "## 4. Feature Selection\n",
    "# --------------------------\n",
    "exclude_cols = {\n",
    "    'date','year','total_pollen','weed','tree','grass', 'ragweed',\n",
    "    'target_asinh','is_spike','spike_z','pollen_ratio_1d_7d'\n",
    "}\n",
    "all_features = [\n",
    "    c for c in train_df.columns \n",
    "    if c not in exclude_cols and train_df[c].dtype in [np.float64, np.int64]\n",
    "]\n",
    "all_features = [c for c in all_features if train_df[c].nunique()>1]\n",
    "\n",
    "X_train, y_train = train_df[all_features], train_df['target_asinh']\n",
    "X_val, y_val = val_df[all_features], val_df['target_asinh']\n",
    "X_test, y_test_original = test_df[all_features], test_df['total_pollen']\n",
    "\n",
    "# --------------------------\n",
    "## 5. Train Spike Classifier\n",
    "# --------------------------\n",
    "clf_params = {\n",
    "    \"n_estimators\":5000,\n",
    "    \"learning_rate\":0.05,\n",
    "    \"num_leaves\":31,\n",
    "    \"min_data_in_leaf\":20,\n",
    "    \"feature_fraction\":0.8,\n",
    "    \"bagging_fraction\":0.8,\n",
    "    \"bagging_freq\":5,\n",
    "    \"random_state\":42,\n",
    "    \"n_jobs\":-1\n",
    "}\n",
    "\n",
    "clf = lgb.LGBMClassifier(**clf_params)\n",
    "clf.fit(\n",
    "    X_train,\n",
    "    train_df['is_spike'],\n",
    "    eval_set=[(X_val, val_df['is_spike'])],\n",
    "    eval_metric='auc',\n",
    "    callbacks=[early_stopping(stopping_rounds=50)]\n",
    ")\n",
    "\n",
    "val_spike_prob = clf.predict_proba(X_val)[:,1]\n",
    "test_spike_prob = clf.predict_proba(X_test)[:,1]\n",
    "print(f\"Classifier val AUC: {roc_auc_score(val_df['is_spike'], val_spike_prob):.3f}\")\n",
    "\n",
    "# --------------------------\n",
    "## 6. Train Regressors\n",
    "# --------------------------\n",
    "reg_params = {\n",
    "    \"n_estimators\":3000,\n",
    "    \"learning_rate\":0.03,\n",
    "    \"num_leaves\":64,\n",
    "    \"min_data_in_leaf\":40,\n",
    "    \"lambda_l1\":1.0,\n",
    "    \"lambda_l2\":1.0,\n",
    "    \"feature_fraction\":0.8,\n",
    "    \"bagging_fraction\":0.8,\n",
    "    \"bagging_freq\":5,\n",
    "    \"random_state\":42,\n",
    "    \"n_jobs\":-1\n",
    "}\n",
    "\n",
    "reg_normal = lgb.LGBMRegressor(**reg_params)\n",
    "reg_normal.fit(\n",
    "    X_train[train_df['is_spike']==0],\n",
    "    y_train[train_df['is_spike']==0],\n",
    "    eval_set=[(X_val, y_val)],\n",
    "    callbacks=[early_stopping(stopping_rounds=50)]\n",
    ")\n",
    "\n",
    "reg_spike = lgb.LGBMRegressor(**reg_params)\n",
    "reg_spike.fit(\n",
    "    X_train[train_df['is_spike']==1],\n",
    "    y_train[train_df['is_spike']==1],\n",
    "    eval_set=[(X_val, y_val)],\n",
    "    callbacks=[early_stopping(stopping_rounds=50)]\n",
    ")\n",
    "\n",
    "# --------------------------\n",
    "## 7. Predictions\n",
    "# --------------------------\n",
    "pred_normal_test = reg_normal.predict(X_test)\n",
    "pred_spike_test = reg_spike.predict(X_test)\n",
    "combined_test_trans = test_spike_prob*pred_spike_test + (1-test_spike_prob)*pred_normal_test\n",
    "y_pred_test = asinh_inverse(combined_test_trans)\n",
    "\n",
    "mae = mean_absolute_error(y_test_original, y_pred_test)\n",
    "rmse = np.sqrt(mean_squared_error(y_test_original, y_pred_test))\n",
    "r2 = r2_score(y_test_original, y_pred_test)\n",
    "print(f\"Test MAE: {mae:.3f}, RMSE: {rmse:.3f}, R^2: {r2:.3f}\")\n",
    "\n",
    "# --------------------------\n",
    "## 8. Feature Importance\n",
    "# --------------------------\n",
    "def print_feature_importance(model, model_name):\n",
    "    importance_df = pd.DataFrame({\n",
    "        \"feature\": model.feature_name_,\n",
    "        \"importance\": model.feature_importances_\n",
    "    }).sort_values(by=\"importance\", ascending=False)\n",
    "    print(f\"\\nFeature Importance for {model_name}:\\n\")\n",
    "    print(importance_df.head(20))\n",
    "    return importance_df\n",
    "\n",
    "clf_importance = print_feature_importance(clf, \"Spike Classifier\")\n",
    "reg_normal_importance = print_feature_importance(reg_normal, \"Normal Regressor\")\n",
    "reg_spike_importance = print_feature_importance(reg_spike, \"Spike Regressor\")\n",
    "\n",
    "# --------------------------\n",
    "## 9. Evaluation Plots\n",
    "# --------------------------\n",
    "def plot_model_evaluation(y_true, y_pred, dates=None, title_prefix=\"Test\"):\n",
    "    # 1. Actual vs Predicted\n",
    "    plt.figure(figsize=(6,6))\n",
    "    plt.scatter(y_true, y_pred, alpha=0.5)\n",
    "    plt.plot([y_true.min(), y_true.max()], [y_true.min(), y_true.max()], 'r--')\n",
    "    plt.xlabel(\"Actual\")\n",
    "    plt.ylabel(\"Predicted\")\n",
    "    plt.title(f\"{title_prefix} Actual vs Predicted\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "    # 2. Residuals vs Predicted\n",
    "    residuals = y_true - y_pred\n",
    "    plt.figure(figsize=(6,4))\n",
    "    plt.scatter(y_pred, residuals, alpha=0.5)\n",
    "    plt.axhline(0, color='r', linestyle='--')\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"Residuals\")\n",
    "    plt.title(f\"{title_prefix} Residuals vs Predicted\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "    # 3. Time Series Plot\n",
    "    if dates is not None:\n",
    "        plt.figure(figsize=(12,4))\n",
    "        plt.plot(dates, y_true, label=\"Actual\", alpha=0.7)\n",
    "        plt.plot(dates, y_pred, label=\"Predicted\", alpha=0.7)\n",
    "        plt.xlabel(\"Date\")\n",
    "        plt.ylabel(\"Total Pollen\")\n",
    "        plt.title(f\"{title_prefix} Actual vs Predicted Over Time\")\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "\n",
    "plot_model_evaluation(y_test_original, y_pred_test, dates=test_df['date'], title_prefix=\"Test\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
